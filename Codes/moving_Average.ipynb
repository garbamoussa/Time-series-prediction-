


## Moving Average Python script

import sys
import json
from kapacitor.udf.agent import Agent, Handler
from kapacitor.udf import udf_pb2

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(name)s: %(message)s')
logger = logging.getLogger()


# Computes the moving average of the data it receives.
# The options it has are:
#    size - the number of data points to keep in the window
#    field - the field to operate on
#    as - the name of the average field, default 'avg'
#
class AvgHandler(Handler):

    class state(object):
        def __init__(self, size):
            self.size = size
            self._window = []
            self._avg = 0.0

        def update(self, value):
            l = float(len(self._window))

            if l == self.size:
                # Window is full, just iteratively update the avg
                self._avg += value/l - self._window[0]/l
                self._window.pop(0)
            else:
                # Window is not full compute the cumulative avg
                self._avg = (value + l*self._avg) / (l + 1)

            self._window.append(value)
            return self._avg



        def snapshot(self):
            return {
                    'size' : self.size,
                    'window' : self._window,
                    'avg' : self._avg,
            }

        def restore(self, data):
            self.size = int(data['size'])
            self._window = [float(d) for d in data['window']]
            self._avg = float(data['avg'])

    def __init__(self, agent):
        self._agent = agent
        self._field = None
        self._size = 0
        self._as = 'avg'
        self._state = {}


    def info(self):
        response = udf_pb2.Response()
        response.info.wants = udf_pb2.STREAM
        response.info.provides = udf_pb2.STREAM
        response.info.options['field'].valueTypes.append(udf_pb2.STRING)
        response.info.options['size'].valueTypes.append(udf_pb2.INT)
        response.info.options['as'].valueTypes.append(udf_pb2.STRING)

        return response

    def init(self, init_req):
        success = True
        msg = ''
        for opt in init_req.options:
            if opt.name == 'field':
                self._field = opt.values[0].stringValue
            elif opt.name == 'size':
                self._size = opt.values[0].intValue
            elif opt.name == 'as':
                self._as = opt.values[0].stringValue

        if self._field is None:
            success = False
            msg += ' must supply field name'
        if self._size == 0:
            success = False
            msg += ' must supply window size'
        if self._as == '':
            success = False
            msg += ' invalid as name'

        response = udf_pb2.Response()
        response.init.success = success
        response.init.error = msg[1:]

        return response

    def snapshot(self):
        data = {}
        for group, state in self._state.iteritems():
            data[group] = state.snapshot()

        response = udf_pb2.Response()
        response.snapshot.snapshot = json.dumps(data)

        return response

    def restore(self, restore_req):
        success = False
        msg = ''
        try:
            data = json.loads(restore_req.snapshot)
            for group, snapshot in data.iteritems():
                self._state[group] = AvgHandler.state(0)
                self._state[group].restore(snapshot)
            success = True
        except Exception as e:
            success = False
            msg = str(e)

        response = udf_pb2.Response()
        response.restore.success = success
        response.restore.error = msg

        return response

    def begin_batch(self, begin_req):
        raise Exception("not supported")

    def point(self, point):
        response = udf_pb2.Response()
        response.point.CopyFrom(point)
        response.point.ClearField('fieldsInt')
        response.point.ClearField('fieldsString')
        response.point.ClearField('fieldsDouble')

        value = point.fieldsDouble[self._field]
        if point.group not in self._state:
            self._state[point.group] = AvgHandler.state(self._size)
        avg = self._state[point.group].update(value)

        response.point.fieldsDouble[self._as] = avg
        self._agent.write_response(response)

    def end_batch(self, end_req):
        raise Exception("not supported")


if __name__ == '__main__':
    a = Agent()
    h = AvgHandler(a)
    a.handler = h

    logger.info("Starting Agent")
    a.start()
    a.wait()
    logger.info("Agent finished")












## Outliers Python scripts




import sys
import json
from kapacitor.udf.agent import Agent, Handler
from kapacitor.udf import udf_pb2

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(name)s: %(message)s')
logger = logging.getLogger()


# Find outliers via the Tukey method. As defined in the README.md
class OutliersHandler(Handler):

    class state(object):
        def __init__(self):
            self._entries = []

        def reset(self):
            self._entries = []

        def update(self, value, point):
            self._entries.append((value, point))

        def outliers(self, scale):
            first, third, lower, upper = self.bounds(scale)
            outliers = []

            # Append lower outliers
            for i in range(first):
                if self._entries[i][0] < lower:
                    outliers.append(self._entries[i][1])
                else:
                    break
            # Append upper outliers
            for i in range(third+1, len(self._entries)):
                if self._entries[i][0] > upper:
                    outliers.append((self._entries[i][1]))

            return outliers

        def bounds(self, scale):
            self._entries = sorted(self._entries, key=lambda x: x[0])
            ml, mr, _ = self.median(self._entries)
            _, first, fq = self.median(self._entries[:mr])
            third, _, tq = self.median(self._entries[ml+1:])
            iqr = tq - fq
            lower = fq - iqr*scale
            upper = tq + iqr*scale
            return first, third, lower, upper

        def median(self, data):
            l = len(data)
            m = l / 2
            if l%2 == 0:
                left = m
                right = m + 1
                median = (data[left][0]+ data[right][0]) / 2.0
            else:
                left = m
                right = m
                median = data[m][0]
            return left, right, median


    def __init__(self, agent):
        self._agent = agent
        self._field = None
        self._scale = 1.5
        self._state = OutliersHandler.state()
        self._begin_response = None


    def info(self):
        response = udf_pb2.Response()
        response.info.wants = udf_pb2.BATCH
        response.info.provides = udf_pb2.BATCH
        response.info.options['field'].valueTypes.append(udf_pb2.STRING)
        response.info.options['scale'].valueTypes.append(udf_pb2.DOUBLE)

        logger.info("info")
        return response

    def init(self, init_req):
        success = True
        msg = ''
        for opt in init_req.options:
            if opt.name == 'field':
                self._field = opt.values[0].stringValue
            elif opt.name == 'scale':
                self._scale = opt.values[0].doubleValue

        if self._field is None:
            success = False
            msg += ' must supply field name'
        if self._scale < 1.0:
            success = False
            msg += ' invalid scale must be >= 1.0'

        response = udf_pb2.Response()
        response.init.success = success
        response.init.error = msg[1:]

        return response

    def snapshot(self):
        response = udf_pb2.Response()
        response.snapshot.snapshot = ''

        return response

    def restore(self, restore_req):
        response = udf_pb2.Response()
        response.restore.success = False
        response.restore.error = 'not implemented'

        return response

    def begin_batch(self, begin_req):
        self._state.reset()

        # Keep copy of begin_batch
        response = udf_pb2.Response()
        response.begin.CopyFrom(begin_req)
        self._begin_response = response

    def point(self, point):
        value = point.fieldsDouble[self._field]
        self._state.update(value, point)

    def end_batch(self, end_req):
        # Get outliers
        outliers = self._state.outliers(self._scale)

        # Send begin batch with count of outliers
        self._begin_response.begin.size = len(outliers)
        self._agent.write_response(self._begin_response)

        response = udf_pb2.Response()
        for outlier in outliers:
            response.point.CopyFrom(outlier)
            self._agent.write_response(response)

        # Send an identical end batch back to Kapacitor
        response.end.CopyFrom(end_req)
        self._agent.write_response(response)


if __name__ == '__main__':
    a = Agent()
    h = OutliersHandler(a)
    a.handler = h

    logger.info("Starting Agent")
    a.start()
    a.wait()
    logger.info("Agent finished")





## Mirror python script 


















import sys
import json
from kapacitor.udf.agent import Agent, Handler, Server
from kapacitor.udf import udf_pb2
import signal

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(name)s: %(message)s')
logger = logging.getLogger()


# Mirrors all points it receives back to Kapacitor
class MirrorHandler(Handler):
    def __init__(self, agent):
        self._agent = agent


    def info(self):
        response = udf_pb2.Response()
        response.info.wants = udf_pb2.STREAM
        response.info.provides = udf_pb2.STREAM
        return response

    def init(self, init_req):
        response = udf_pb2.Response()
        response.init.success = True
        return response

    def snapshot(self):
        response = udf_pb2.Response()
        response.snapshot.snapshot = ''
        return response

    def restore(self, restore_req):
        response = udf_pb2.Response()
        response.restore.success = False
        response.restore.error = 'not implemented'
        return response

    def begin_batch(self, begin_req):
        raise Exception("not supported")

    def point(self, point):
        response = udf_pb2.Response()
        response.point.CopyFrom(point)
        self._agent.write_response(response, True)

    def end_batch(self, end_req):
        raise Exception("not supported")

class accepter(object):
    _count = 0
    def accept(self, conn, addr):
        self._count += 1
        a = Agent(conn, conn)
        h = MirrorHandler(a)
        a.handler = h

        logger.info("Starting Agent for connection %d", self._count)
        a.start()
        a.wait()
        logger.info("Agent finished connection %d",self._count)

if __name__ == '__main__':
    path = "/tmp/mirror.sock"
    if len(sys.argv) == 2:
        path = sys.argv[1]
    server = Server(path, accepter())
    logger.info("Started server")
    server.serve()







